import time
import random
import sys
import re
import os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

BASE_URL = "https://govolunteerhcmc.vn"
FALLBACK_IMAGE_URL = "https://govolunteerhcmc.vn/wp-content/uploads/2024/02/logo-gv-tron.png"

def setup_driver():
    """C·∫•u h√¨nh Chrome Driver m·ªôt c√°ch ƒë√°ng tin c·∫≠y cho m√¥i tr∆∞·ªùng Render."""
    chrome_options = Options()
    
    # --- S·ª¨A L·ªñI QUAN TR·ªåNG NH·∫§T ---
    # N·∫øu kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng, h√£y s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
    # m√† buildpack th∆∞·ªùng c√†i ƒë·∫∑t. ƒêi·ªÅu n√†y l√†m cho code ·ªïn ƒë·ªãnh h∆°n.
    chrome_bin = os.environ.get("GOOGLE_CHROME_BIN", "/app/.apt/usr/bin/google-chrome")
    driver_path = os.environ.get("CHROMEDRIVER_PATH", "/app/.chromedriver/bin/chromedriver")
    
    print(f"‚ÑπÔ∏è S·ª≠ d·ª•ng Chrome t·∫°i: {chrome_bin}")
    print(f"‚ÑπÔ∏è S·ª≠ d·ª•ng Chromedriver t·∫°i: {driver_path}")
    
    chrome_options.binary_location = chrome_bin
    
    # C√°c t√πy ch·ªçn b·∫Øt bu·ªôc ƒë·ªÉ ch·∫°y trong m√¥i tr∆∞·ªùng container (server)
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    print("üöÄ ƒêang kh·ªüi t·∫°o Selenium Driver v·ªõi ƒë∆∞·ªùng d·∫´n ƒë√£ x√°c ƒë·ªãnh...")
    try:
        service = Service(executable_path=driver_path)
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(30)
        print("‚úÖ Selenium Driver ƒë√£ s·∫µn s√†ng.")
        return driver
    except Exception as e:
        print(f"‚ùå L·ªñI KH·ªûI T·∫†O DRIVER: {e}", file=sys.stderr)
        return None

def get_high_res_image_url(url: str):
    if not url: return FALLBACK_IMAGE_URL
    return re.sub(r'-\d{2,4}x\d{2,4}(?=\.\w+$)', '', url)

def scrape_news():
    driver = setup_driver()
    if not driver: return None

    try:
        driver.get(BASE_URL)
        time.sleep(5)
        soup = BeautifulSoup(driver.page_source, "lxml")
        sections = []
        for sec in soup.select("section.elementor-section.elementor-top-section"):
            h2 = sec.select_one("h2.elementor-heading-title.elementor-size-default")
            if not h2 or not h2.text.strip(): continue
            category = h2.text.strip()
            articles = []
            for post in sec.select("article.elementor-post"):
                a = post.select_one("h3.elementor-post__title a")
                if not a or not a.get('href', '').startswith(BASE_URL): continue
                img = post.select_one(".elementor-post__thumbnail img")
                image_url = get_high_res_image_url(img['src']) if img and img.get('src') else FALLBACK_IMAGE_URL
                ex = post.select_one(".elementor-post__excerpt p")
                excerpt = ex.text.strip() if ex else None
                articles.append({"title": a.text.strip(), "link": a['href'], "imageUrl": image_url, "excerpt": excerpt})
            if articles:
                sections.append({"category": category, "articles": list({article['link']: article for article in articles}.values())})
        print(f"‚úÖ L·∫•y d·ªØ li·ªáu trang ch·ªß th√†nh c√¥ng, t√¨m th·∫•y {len(sections)} m·ª•c.")
        return list({section['category']: section for section in sections}.values())
    except Exception as e:
        print(f"‚ùå L·ªói khi scraping trang ch·ªß: {e}", file=sys.stderr)
        return None
    finally:
        if driver: driver.quit()

def scrape_article_content(article_url: str):
    driver = setup_driver()
    if not driver: return None
    try:
        driver.get(article_url)
        time.sleep(3)
        content_div = driver.find_element(by="css selector", value=".elementor-widget-theme-post-content")
        html_content = content_div.get_attribute('outerHTML')
        print("‚úÖ L·∫•y n·ªôi dung b√†i vi·∫øt th√†nh c√¥ng!")
        return html_content
    except Exception as e:
        print(f"‚ùå L·ªói khi scraping b√†i vi·∫øt: {e}", file=sys.stderr)
        return None
    finally:
        if driver: driver.quit()
