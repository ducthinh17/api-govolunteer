
import time
import random
import sys
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

BASE_URL = "https://govolunteerhcmc.vn"
FALLBACK_IMAGE_URL = "https://govolunteerhcmc.vn/wp-content/uploads/2024/02/logo-gv-tron.png"

def setup_driver():
    """
    C·∫•u h√¨nh v√† kh·ªüi t·∫°o m·ªôt Chrome driver ·ªü ch·∫ø ƒë·ªô headless.
    Ch·∫ø ƒë·ªô n√†y r·∫•t quan tr·ªçng ƒë·ªÉ ch·∫°y tr√™n server kh√¥ng c√≥ giao di·ªán ƒë·ªì h·ªça.
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f'user-agent={random.choice(["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"])}')
    
    print("üöÄ ƒêang kh·ªüi t·∫°o Selenium Driver...")
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(45) # TƒÉng th·ªùi gian ch·ªù t·∫£i trang l√™n 45 gi√¢y
        print("‚úÖ Selenium Driver ƒë√£ s·∫µn s√†ng.")
        return driver
    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG KHI KH·ªûI T·∫†O DRIVER: {e}", file=sys.stderr)
        return None

def get_high_res_image_url(url: str):
    """
    Lo·∫°i b·ªè c√°c h·∫≠u t·ªë k√≠ch th∆∞·ªõc c·ªßa WordPress (v√≠ d·ª•: "-300x192")
    kh·ªèi URL h√¨nh ·∫£nh ƒë·ªÉ l·∫•y phi√™n b·∫£n c√≥ ƒë·ªô ph√¢n gi·∫£i cao nh·∫•t.
    """
    if not url: return FALLBACK_IMAGE_URL
    return url.replace(/-\d{2,4}x\d{2,4}(?=\.\w+$)/, "")

def scrape_news():
    """
    Scrapes news articles from the GoVolunteerHCMC main page using Selenium.
    """
    driver = setup_driver()
    if not driver:
        return None

    try:
        print(f"üåç ƒêang truy c·∫≠p trang ch·ªß: {BASE_URL}")
        driver.get(BASE_URL)
        time.sleep(5)  # ƒê·ª£i JavaScript t·∫£i
        
        soup = BeautifulSoup(driver.page_source, "lxml")
        sections = []
        
        for sec in soup.select("section.elementor-section.elementor-top-section"):
            h2 = sec.select_one("h2.elementor-heading-title.elementor-size-default")
            if not h2 or not h2.text.strip():
                continue
            
            category = h2.text.strip()
            articles = []
            
            for post in sec.select("article.elementor-post"):
                a = post.select_one("h3.elementor-post__title a")
                if not a or not a.get('href', '').startswith(BASE_URL):
                    continue

                title = a.text.strip()
                link = a['href']
                
                img = post.select_one(".elementor-post__thumbnail img")
                image_url = get_high_res_image_url(img['src']) if img and img.get('src') else FALLBACK_IMAGE_URL
                
                ex = post.select_one(".elementor-post__excerpt p")
                excerpt = ex.text.strip() if ex else None
                
                articles.append({
                    "title": title,
                    "link": link,
                    "imageUrl": image_url,
                    "excerpt": excerpt,
                })
            
            if articles:
                unique_articles = list({article['link']: article for article in articles}.values())
                sections.append({
                    "category": category,
                    "articles": unique_articles,
                })
        
        print(f"‚úÖ L·∫•y d·ªØ li·ªáu trang ch·ªß th√†nh c√¥ng, t√¨m th·∫•y {len(sections)} m·ª•c.")
        return list({section['category']: section for section in sections}.values())

    except Exception as e:
        print(f"‚ùå L·ªói khi scraping trang ch·ªß: {e}", file=sys.stderr)
        return None
    finally:
        driver.quit()
        print("üö™ ƒê√£ ƒë√≥ng Selenium Driver c·ªßa t√°c v·ª• /news.")

def scrape_article_content(article_url: str):
    """
    Scrapes the content of a single article using Selenium.
    """
    driver = setup_driver()
    if not driver:
        return None

    try:
        print(f"üìÑ ƒêang truy c·∫≠p b√†i vi·∫øt: {article_url}")
        driver.get(article_url)
        time.sleep(3)
        
        print("üîç ƒêang t√¨m ki·∫øm th·∫ª div n·ªôi dung '.elementor-widget-theme-post-content'...")
        content_div = driver.find_element(by="css selector", value=".elementor-widget-theme-post-content")
        
        html_content = content_div.get_attribute('outerHTML')
        print("‚úÖ T√åM TH·∫§Y V√Ä L·∫§Y N·ªòI DUNG TH√ÄNH C√îNG!")
        return html_content
            
    except TimeoutException:
        print(f"‚ùå L·ªñI TIMEOUT: Trang web m·∫•t qu√° nhi·ªÅu th·ªùi gian ƒë·ªÉ t·∫£i. ({article_url})", file=sys.stderr)
        return None
    except Exception as e:
        print(f"‚ùå L·ªñI KH√îNG T√åM TH·∫§Y ELEMENT ho·∫∑c l·ªói kh√°c: {e}", file=sys.stderr)
        return None
    finally:
        driver.quit()
        print("üö™ ƒê√≥ng Selenium Driver c·ªßa t√°c v·ª• /article.")

# ----------------------------------------------------------------------
# FILE: main.py (PHI√äN B·∫¢N CHUY√äN NGHI·ªÜP V√Ä HO√ÄN CH·ªàNH)
#
# C·∫≠p nh·∫≠t file n√†y ƒë·ªÉ tr·∫£ v·ªÅ th√¥ng b√°o l·ªói chi ti·∫øt h∆°n v√† r√µ r√†ng h∆°n.
# ----------------------------------------------------------------------

import time
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
# ƒê·ªïi t√™n import ƒë·ªÉ d·ªÖ ph√¢n bi·ªát
from scraper import scrape_news as fetch_news_from_source
from scraper import scrape_article_content as fetch_article_from_source

app = FastAPI(
    title="GoVolunteer Scraper API",
    description="API chuy√™n nghi·ªáp ƒë·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ GoVolunteerHCMC.",
    version="3.0.0",
)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["GET"], allow_headers=["*"])

cache = { "news_data": None, "last_fetched": 0 }
CACHE_DURATION_SECONDS = 1800 # Cache trong 30 ph√∫t

@app.get("/", summary="Ki·ªÉm tra tr·∫°ng th√°i API")
def read_root():
    return {"status": "online", "message": "Ch√†o m·ª´ng ƒë·∫øn v·ªõi API GoVolunteer!"}

@app.get("/news", summary="L·∫•y danh s√°ch t·∫•t c·∫£ tin t·ª©c (c√≥ cache)")
def get_all_news():
    current_time = time.time()
    if cache["news_data"] and (current_time - cache["last_fetched"] < CACHE_DURATION_SECONDS):
        print("‚úÖ Tr·∫£ v·ªÅ d·ªØ li·ªáu t·ª´ cache.")
        return cache["news_data"]
    
    print("‚ôªÔ∏è Cache ƒë√£ h·∫øt h·∫°n. B·∫Øt ƒë·∫ßu scrape d·ªØ li·ªáu m·ªõi...")
    data = fetch_news_from_source()
    
    if not data:
        raise HTTPException(status_code=503, detail="Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ trang ch·ªß GoVolunteer. Server c√≥ th·ªÉ ƒëang b·∫≠n ho·∫∑c trang web kh√¥ng ph·∫£n h·ªìi.")
    
    cache["news_data"] = data
    cache["last_fetched"] = current_time
    print("üíæ ƒê√£ c·∫≠p nh·∫≠t cache v·ªõi d·ªØ li·ªáu m·ªõi.")
    return data

@app.get("/article", summary="L·∫•y n·ªôi dung chi ti·∫øt c·ªßa m·ªôt b√†i vi·∫øt")
def get_article_detail(url: str):
    if not url or not url.startswith(BASE_URL):
        raise HTTPException(status_code=400, detail=f"URL kh√¥ng h·ª£p l·ªá.")
    
    content = fetch_article_from_source(url)
    
    if content is None:
        raise HTTPException(status_code=503, detail="Server ƒë√£ g·∫∑p l·ªói khi c·ªë g·∫Øng l·∫•y n·ªôi dung b√†i vi·∫øt. Nguy√™n nh√¢n c√≥ th·ªÉ do h·∫øt th·ªùi gian ch·ªù ho·∫∑c kh√¥ng t√¨m th·∫•y n·ªôi dung tr√™n trang.")
        
    return {"html_content": content}
