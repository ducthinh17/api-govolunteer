import requests
from bs4 import BeautifulSoup
import re
import sys

# --- C·∫•u h√¨nh chung ---
BASE_URL = "https://govolunteerhcmc.vn"
FALLBACK_IMAGE_URL = "https://govolunteerhcmc.vn/wp-content/uploads/2024/02/logo-gv-tron.png"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
    'Referer': 'https://www.google.com/'
}

def get_high_res_image_url(url: str):
    """Lo·∫°i b·ªè c√°c h·∫≠u t·ªë k√≠ch th∆∞·ªõc ·∫£nh (-150x150, -300x200, v.v.) ƒë·ªÉ l·∫•y ·∫£nh g·ªëc ch·∫•t l∆∞·ª£ng cao."""
    if not url:
        return FALLBACK_IMAGE_URL
    # Bi·ªÉu th·ª©c ch√≠nh quy n√†y s·∫Ω t√¨m v√† thay th·∫ø c√°c chu·ªói nh∆∞ "-123x456" ·ªü cu·ªëi t√™n file (tr∆∞·ªõc ph·∫ßn m·ªü r·ªông)
    return re.sub(r'-\d{2,4}x\d{2,4}(?=\.\w+$)', '', url)

# --- PHI√äN B·∫¢N `requests` CHO /NEWS (ƒê√É T·ªêI ∆ØU) ---
def scrape_news():
    """
    S·ª≠ d·ª•ng `requests` ƒë·ªÉ l·∫•y d·ªØ li·ªáu trang ch·ªß m·ªôt c√°ch nhanh ch√≥ng.
    ƒê√£ c·∫≠p nh·∫≠t logic ƒë·ªÉ l·∫•y c·∫£ section kh√¥ng c√≥ ti√™u ƒë·ªÅ (d√†nh cho Swiper/B·∫£ng tin).
    """
    print("üöÄ S·ª≠ d·ª•ng `requests` ƒë·ªÉ l·∫•y d·ªØ li·ªáu /news...")
    try:
        response = requests.get(BASE_URL, headers=HEADERS, timeout=15)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"‚ùå L·ªói khi d√πng requests cho /news: {e}", file=sys.stderr)
        return []

    soup = BeautifulSoup(response.text, "lxml")
    sections = []

    for sec in soup.select("section.elementor-section.elementor-top-section"):
        # B∆Ø·ªöC 1: T√¨m t·∫•t c·∫£ b√†i vi·∫øt trong section tr∆∞·ªõc
        articles = []
        for post in sec.select("article.elementor-post"):
            a_tag = post.select_one("h3.elementor-post__title a")
            if not a_tag or not a_tag.get('href', '').startswith(BASE_URL):
                continue

            img_tag = post.select_one(".elementor-post__thumbnail img")
            image_url = get_high_res_image_url(img_tag.get('src')) if img_tag and img_tag.get('src') else FALLBACK_IMAGE_URL

            excerpt_tag = post.select_one(".elementor-post__excerpt p")
            excerpt = excerpt_tag.text.strip() if excerpt_tag else None

            articles.append({
                "title": a_tag.text.strip(),
                "link": a_tag['href'],
                "imageUrl": image_url,
                "excerpt": excerpt,
            })

        # B∆Ø·ªöC 2: CH·ªà x·ª≠ l√Ω n·∫øu section n√†y th·ª±c s·ª± c√≥ b√†i vi·∫øt
        if not articles:
            continue

        # B∆Ø·ªöC 3: B√¢y gi·ªù m·ªõi t√¨m ti√™u ƒë·ªÅ. N·∫øu kh√¥ng c√≥, g√°n t√™n m·∫∑c ƒë·ªãnh.
        h2 = sec.select_one("h2.elementor-heading-title.elementor-size-default")
        
        # ----- ƒê√ÇY L√Ä THAY ƒê·ªîI QUAN TR·ªåNG NH·∫§T -----
        # Logic n√†y ƒë·∫£m b·∫£o section kh√¥ng c√≥ ti√™u ƒë·ªÅ (nh∆∞ slider) v·∫´n ƒë∆∞·ª£c l·∫•y
        # v√† ƒë∆∞·ª£c ƒë·∫∑t t√™n l√† "B·∫¢NG TIN T√åNH NGUY·ªÜN" ƒë·ªÉ frontend x·ª≠ l√Ω.
        category_name = h2.text.strip() if h2 and h2.text.strip() else "B·∫¢NG TIN T√åNH NGUY·ªÜN"
        
        unique_articles = list({article['link']: article for article in articles}.values())
        sections.append({"category": category_name, "articles": unique_articles})

    print(f"‚úÖ L·∫•y d·ªØ li·ªáu /news th√†nh c√¥ng, t√¨m th·∫•y {len(sections)} m·ª•c.")
    
    # Lo·∫°i b·ªè c√°c category tr√πng l·∫∑p (n·∫øu c√≥), gi·ªØ l·∫°i b·∫£n c√≥ d·ªØ li·ªáu
    final_sections = list({section['category']: section for section in sections}.values())
    
    return final_sections


# --- PHI√äN B·∫¢N `requests` CHO /ARTICLE ---
def scrape_article_with_requests(article_url: str):
    """S·ª≠ d·ª•ng requests v√† BeautifulSoup ƒë·ªÉ l·∫•y n·ªôi dung b√†i vi·∫øt m·ªôt c√°ch hi·ªáu qu·∫£."""
    print(f"üöÄ S·ª≠ d·ª•ng `requests` ƒë·ªÉ l·∫•y d·ªØ li·ªáu b√†i vi·∫øt: {article_url}")
    try:
        response = requests.get(article_url, headers=HEADERS, timeout=20)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "lxml")

        # Selector n√†y nh·∫Øm ƒë·∫øn container b√™n trong widget n·ªôi dung c·ªßa Elementor
        # ƒë·ªÉ l·∫•y ch√≠nh x√°c ph·∫ßn th√¢n b√†i vi·∫øt
        content_div = soup.select_one(".elementor-widget-theme-post-content .elementor-widget-container")

        if not content_div:
            print("‚ùå Kh√¥ng t√¨m th·∫•y th·∫ª div ch·ª©a n·ªôi dung (.elementor-widget-theme-post-content).", file=sys.stderr)
            return None

        html_content = str(content_div)
        print("‚úÖ L·∫•y n·ªôi dung b√†i vi·∫øt th√†nh c√¥ng!")
        return html_content

    except requests.RequestException as e:
        print(f"‚ùå L·ªói khi d√πng requests cho b√†i vi·∫øt: {e}", file=sys.stderr)
        return None
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i vi·∫øt: {e}", file=sys.stderr)
        return None
# --- Placeholder functions for new routes ---

def scrape_chuong_trinh_chien_dich_du_an():
    print("Scraping chuong-trinh-chien-dich-du-an")
    return []

def scrape_skills():
    print("Scraping /skills")
    return []

def scrape_ideas():
    print("Scraping /ideas")
    return []
