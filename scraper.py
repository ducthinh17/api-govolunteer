import time
import random
import sys
import re
import os # Th√™m th∆∞ vi·ªán os
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

BASE_URL = "https://govolunteerhcmc.vn"
FALLBACK_IMAGE_URL = "https://govolunteerhcmc.vn/wp-content/uploads/2024/02/logo-gv-tron.png"

def setup_driver():
    """C·∫•u h√¨nh Chrome Driver cho m√¥i tr∆∞·ªùng Render."""
    chrome_options = Options()
    # C√°c ƒë∆∞·ªùng d·∫´n n√†y l√† ti√™u chu·∫©n khi s·ª≠ d·ª•ng Heroku/Render buildpacks
    chrome_options.binary_location = os.environ.get("GOOGLE_CHROME_BIN", "/app/.apt/usr/bin/google-chrome")
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument(f'user-agent={random.choice(["Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"])}')
    
    print("üöÄ ƒêang kh·ªüi t·∫°o Selenium Driver tr√™n m√¥i tr∆∞·ªùng Render...")
    try:
        # ƒê∆∞·ªùng d·∫´n t·ªõi chromedriver do buildpack cung c·∫•p
        driver_path = os.environ.get("CHROMEDRIVER_PATH", "/app/.chromedriver/bin/chromedriver")
        service = Service(executable_path=driver_path)
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(45)
        print("‚úÖ Selenium Driver ƒë√£ s·∫µn s√†ng.")
        return driver
    except Exception as e:
        print(f"‚ùå L·ªñI NGHI√äM TR·ªåNG KHI KH·ªûI T·∫†O DRIVER: {e}", file=sys.stderr)
        return None

def get_high_res_image_url(url: str):
    """L·∫•y ·∫£nh g·ªëc c√≥ ƒë·ªô ph√¢n gi·∫£i cao."""
    if not url: return FALLBACK_IMAGE_URL
    return re.sub(r'-\d{2,4}x\d{2,4}(?=\.\w+$)', '', url)

def scrape_news():
    """L·∫•y d·ªØ li·ªáu t·ª´ trang ch·ªß."""
    driver = setup_driver()
    if not driver:
        return None

    try:
        print(f"üåç ƒêang truy c·∫≠p trang ch·ªß: {BASE_URL}")
        driver.get(BASE_URL)
        time.sleep(5)
        
        soup = BeautifulSoup(driver.page_source, "lxml")
        sections = []
        
        for sec in soup.select("section.elementor-section.elementor-top-section"):
            h2 = sec.select_one("h2.elementor-heading-title.elementor-size-default")
            if not h2 or not h2.text.strip():
                continue
            
            category = h2.text.strip()
            articles = []
            
            for post in sec.select("article.elementor-post"):
                a = post.select_one("h3.elementor-post__title a")
                if not a or not a.get('href', '').startswith(BASE_URL):
                    continue

                img = post.select_one(".elementor-post__thumbnail img")
                image_url = get_high_res_image_url(img['src']) if img and img.get('src') else FALLBACK_IMAGE_URL
                
                ex = post.select_one(".elementor-post__excerpt p")
                excerpt = ex.text.strip() if ex else None
                
                articles.append({
                    "title": a.text.strip(),
                    "link": a['href'],
                    "imageUrl": image_url,
                    "excerpt": excerpt,
                })
            
            if articles:
                unique_articles = list({article['link']: article for article in articles}.values())
                sections.append({
                    "category": category,
                    "articles": unique_articles,
                })
        
        print(f"‚úÖ L·∫•y d·ªØ li·ªáu trang ch·ªß th√†nh c√¥ng, t√¨m th·∫•y {len(sections)} m·ª•c.")
        return list({section['category']: section for section in sections}.values())

    except Exception as e:
        print(f"‚ùå L·ªói khi scraping trang ch·ªß: {e}", file=sys.stderr)
        return None
    finally:
        driver.quit()
        print("üö™ ƒê√£ ƒë√≥ng Selenium Driver c·ªßa t√°c v·ª• /news.")

def scrape_article_content(article_url: str):
    """L·∫•y n·ªôi dung chi ti·∫øt c·ªßa m·ªôt b√†i vi·∫øt."""
    driver = setup_driver()
    if not driver:
        return None

    try:
        print(f"ÔøΩ ƒêang truy c·∫≠p b√†i vi·∫øt: {article_url}")
        driver.get(article_url)
        time.sleep(3)
        
        print("üîç ƒêang t√¨m ki·∫øm th·∫ª div n·ªôi dung '.elementor-widget-theme-post-content'...")
        content_div = driver.find_element(by="css selector", value=".elementor-widget-theme-post-content")
        
        html_content = content_div.get_attribute('outerHTML')
        print("‚úÖ T√åM TH·∫§Y V√Ä L·∫§Y N·ªòI DUNG TH√ÄNH C√îNG!")
        return html_content
            
    except TimeoutException:
        print(f"‚ùå L·ªñI TIMEOUT: Trang web m·∫•t qu√° nhi·ªÅu th·ªùi gian ƒë·ªÉ t·∫£i. ({article_url})", file=sys.stderr)
        return None
    except Exception as e:
        print(f"‚ùå L·ªñI KH√îNG T√åM TH·∫§Y ELEMENT ho·∫∑c l·ªói kh√°c: {e}", file=sys.stderr)
        return None
    finally:
        driver.quit()
        print("üö™ ƒê√≥ng Selenium Driver c·ªßa t√°c v·ª• /article.")
